"""
PH·∫¶N 7 ‚Äì Safety & Ethics Layer

M·ª•c ti√™u:
- Tr√°nh l·ªùi khuy√™n nguy hi·ªÉm
- Gi·ªØ ranh gi·ªõi gi·ªØa tr·ªã li·ªáu & ch·∫©n ƒëo√°n
- X·ª≠ l√Ω tr∆∞·ªùng h·ª£p nguy c∆° (self-harm)
- H·∫≠u ki·ªÉm n·ªôi dung tr∆∞·ªõc khi g·ª≠i
"""

import logging
import re
from typing import Dict, Optional, List, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class RiskLevel(Enum):
    """M·ª©c ƒë·ªô nguy c∆°."""
    NONE = "none"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ContentType(Enum):
    """Lo·∫°i n·ªôi dung c·∫ßn ki·ªÉm tra."""
    USER_MESSAGE = "user_message"
    BOT_RESPONSE = "bot_response"
    PROACTIVE_QUESTION = "proactive_question"


@dataclass
class SafetyCheck:
    """K·∫øt qu·∫£ ki·ªÉm tra an to√†n."""
    is_safe: bool
    risk_level: RiskLevel
    flags: List[str]
    requires_action: bool
    action_type: Optional[str]  # "crisis_response", "add_disclaimer", "block", "modify"
    modified_content: Optional[str]
    reason: str


class SafetyEthicsLayer:
    """
    Layer ƒë·∫£m b·∫£o an to√†n v√† ƒë·∫°o ƒë·ª©c.
    Ki·ªÉm tra v√† x·ª≠ l√Ω n·ªôi dung nguy hi·ªÉm.
    """
    
    # Crisis keywords - m·ª©c ƒë·ªô cao nh·∫•t
    CRISIS_KEYWORDS = {
        "critical": [
            "t·ª± t·ª≠", "mu·ªën ch·∫øt", "k·∫øt th√∫c cu·ªôc s·ªëng", "kh√¥ng mu·ªën s·ªëng",
            "t·ª± l√†m ƒëau", "c·∫Øt tay", "t·ª± h·∫°i", "u·ªëng thu·ªëc qu√° li·ªÅu",
            "suicide", "kill myself", "end my life", "self-harm",
            "overdose", "want to die", "better off dead"
        ],
        "high": [
            "tuy·ªát v·ªçng", "v√¥ v·ªçng", "kh√¥ng c√≥ l√Ω do s·ªëng", "kh√¥ng ai c·∫ßn",
            "s·∫Ω bi·∫øn m·∫•t", "m·ªçi ng∆∞·ªùi s·∫Ω t·ªët h∆°n khi kh√¥ng c√≥ t√¥i",
            "hopeless", "worthless", "burden to everyone",
            "no reason to live", "can't go on"
        ],
        "medium": [
            "kh√¥ng mu·ªën th·ª©c d·∫≠y", "m·ªát m·ªèi v·ªõi cu·ªôc s·ªëng", "tr·∫ßm c·∫£m n·∫∑ng",
            "kh√¥ng th·ªÉ ti·∫øp t·ª•c", "qu√° ƒëau ƒë·ªõn",
            "tired of living", "can't take it anymore"
        ]
    }
    
    # C√¢u h·ªèi kh√¥ng n√™n h·ªèi ch·ªß ƒë·ªông
    PROHIBITED_PROACTIVE_QUESTIONS = [
        "b·ªã l·∫°m d·ª•ng", "l·∫°m d·ª•ng t√¨nh d·ª•c", "b·∫°o l·ª±c gia ƒë√¨nh",
        "ƒëang d√πng thu·ªëc t√¢m th·∫ßn", "ƒë√£ t·ª´ng t·ª± t·ª≠", "c√≥ √Ω ƒë·ªãnh t·ª± t·ª≠",
        "abuse", "trauma details", "medication names"
    ]
    
    # N·ªôi dung c·∫ßn disclaimer
    DISCLAIMER_TRIGGERS = [
        "ch·∫©n ƒëo√°n", "b·ªánh", "r·ªëi lo·∫°n", "thu·ªëc", "ƒëi·ªÅu tr·ªã y t·∫ø",
        "diagnosis", "disorder", "medication", "treatment"
    ]
    
    # Templates cho responses
    CRISIS_RESPONSE = """M√¨nh r·∫•t lo l·∫Øng v·ªÅ nh·ªØng g√¨ b·∫°n ƒëang chia s·∫ª. 
C·∫£m x√∫c c·ªßa b·∫°n l√† th·∫≠t v√† quan tr·ªçng, v√† m√¨nh mu·ªën b·∫°n ƒë∆∞·ª£c an to√†n.

üÜò **Vui l√≤ng li√™n h·ªá ngay:**
- ƒê∆∞·ªùng d√¢y n√≥ng h·ªó tr·ª£ t√¢m l√Ω: 1800 599 920 (mi·ªÖn ph√≠, 24/7)
- T·ªïng ƒë√†i s·ª©c kh·ªèe t√¢m th·∫ßn: 1900 0027
- Trong tr∆∞·ªùng h·ª£p kh·∫©n c·∫•p: G·ªçi 115

üíö Nh·ªØng ng∆∞·ªùi t∆∞ v·∫•n chuy√™n nghi·ªáp ƒë∆∞·ª£c ƒë√†o t·∫°o ƒë·ªÉ h·ªó tr·ª£ b·∫°n qua giai ƒëo·∫°n n√†y.

B·∫°n kh√¥ng ƒë∆°n ƒë·ªôc. M√¨nh ·ªü ƒë√¢y l·∫Øng nghe, nh∆∞ng nh·ªØng chuy√™n gia c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n t·ªët h∆°n."""

    SOFT_DISCLAIMER = """
üí° *L∆∞u √Ω: M√¨nh l√† tr·ª£ l√Ω AI h·ªó tr·ª£ s·ª©c kh·ªèe tinh th·∫ßn, kh√¥ng thay th·∫ø t∆∞ v·∫•n y t·∫ø chuy√™n nghi·ªáp. 
N·∫øu b·∫°n c·∫ßn h·ªó tr·ª£ chuy√™n s√¢u, h√£y li√™n h·ªá chuy√™n gia t√¢m l√Ω.*"""

    BOUNDARY_REMINDER = """
M√¨nh kh√¥ng c√≥ kh·∫£ nƒÉng ƒë∆∞a ra ch·∫©n ƒëo√°n y t·∫ø hay k√™ ƒë∆°n thu·ªëc. 
ƒê·ªÉ ƒë∆∞·ª£c ƒë√°nh gi√° ch√≠nh x√°c, b·∫°n n√™n g·∫∑p b√°c sƒ© ho·∫∑c chuy√™n gia t√¢m l√Ω.
M√¨nh c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ c√°ch ƒë·ªëi ph√≥ v√† nh·ªØng b∆∞·ªõc ti·∫øp theo."""

    def __init__(self, model=None):
        """
        Kh·ªüi t·∫°o Safety & Ethics Layer.
        
        Args:
            model: Gemini model cho content review (optional)
        """
        self.model = model
        logger.info("‚úì SafetyEthicsLayer initialized")
    
    def check_user_message(self, message: str) -> SafetyCheck:
        """
        Ki·ªÉm tra tin nh·∫Øn ng∆∞·ªùi d√πng.
        
        Args:
            message: Tin nh·∫Øn ng∆∞·ªùi d√πng
            
        Returns:
            SafetyCheck v·ªõi k·∫øt qu·∫£
        """
        flags = []
        risk_level = RiskLevel.NONE
        action_type = None
        
        message_lower = message.lower()
        
        # Check critical keywords
        for keyword in self.CRISIS_KEYWORDS["critical"]:
            if keyword in message_lower:
                flags.append(f"Critical: {keyword}")
                risk_level = RiskLevel.CRITICAL
                action_type = "crisis_response"
                break
        
        # Check high risk if not critical
        if risk_level != RiskLevel.CRITICAL:
            for keyword in self.CRISIS_KEYWORDS["high"]:
                if keyword in message_lower:
                    flags.append(f"High risk: {keyword}")
                    risk_level = RiskLevel.HIGH
                    action_type = "crisis_response"
                    break
        
        # Check medium risk
        if risk_level == RiskLevel.NONE:
            for keyword in self.CRISIS_KEYWORDS["medium"]:
                if keyword in message_lower:
                    flags.append(f"Medium risk: {keyword}")
                    risk_level = RiskLevel.MEDIUM
                    action_type = "add_disclaimer"
                    break
        
        # Determine if safe
        is_safe = risk_level in [RiskLevel.NONE, RiskLevel.LOW, RiskLevel.MEDIUM]
        requires_action = risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]
        
        return SafetyCheck(
            is_safe=is_safe,
            risk_level=risk_level,
            flags=flags,
            requires_action=requires_action,
            action_type=action_type,
            modified_content=self.CRISIS_RESPONSE if action_type == "crisis_response" else None,
            reason="; ".join(flags) if flags else "No concerns detected"
        )
    
    def check_bot_response(self, response: str, user_message: str = "") -> SafetyCheck:
        """
        H·∫≠u ki·ªÉm ph·∫£n h·ªìi c·ªßa bot tr∆∞·ªõc khi g·ª≠i.
        
        Args:
            response: Ph·∫£n h·ªìi c·ªßa bot
            user_message: Tin nh·∫Øn ng∆∞·ªùi d√πng (ƒë·ªÉ context)
            
        Returns:
            SafetyCheck v·ªõi k·∫øt qu·∫£ v√† n·ªôi dung ƒë√£ s·ª≠a n·∫øu c·∫ßn
        """
        flags = []
        modified = response
        needs_disclaimer = False
        
        response_lower = response.lower()
        
        # Check for inappropriate content
        inappropriate_patterns = [
            (r"b·∫°n b·ªã (b·ªánh|r·ªëi lo·∫°n|m·∫Øc)", "Potential diagnosis language"),
            (r"b·∫°n n√™n u·ªëng thu·ªëc", "Medication advice"),
            (r"b·∫°n ch·∫Øc ch·∫Øn (c√≥|b·ªã)", "Definitive diagnosis"),
            (r"theo ch·∫©n ƒëo√°n", "Diagnostic language"),
        ]
        
        for pattern, reason in inappropriate_patterns:
            if re.search(pattern, response_lower):
                flags.append(reason)
        
        # Check if disclaimer needed
        for trigger in self.DISCLAIMER_TRIGGERS:
            if trigger in response_lower:
                needs_disclaimer = True
                break
        
        # Add disclaimer if needed
        if needs_disclaimer and self.SOFT_DISCLAIMER not in response:
            modified = response + "\n" + self.SOFT_DISCLAIMER
        
        # Check for harmful advice
        harmful_patterns = [
            r"kh√¥ng c·∫ßn ƒëi (b√°c sƒ©|kh√°m)",
            r"t·ª± ƒëi·ªÅu tr·ªã",
            r"b·ªè thu·ªëc",
            r"kh√¥ng c·∫ßn lo",  # Dismissive
        ]
        
        for pattern in harmful_patterns:
            if re.search(pattern, response_lower):
                flags.append(f"Potentially harmful: {pattern}")
        
        is_safe = len(flags) == 0
        
        return SafetyCheck(
            is_safe=is_safe,
            risk_level=RiskLevel.LOW if flags else RiskLevel.NONE,
            flags=flags,
            requires_action=needs_disclaimer or len(flags) > 0,
            action_type="modify" if modified != response else None,
            modified_content=modified if modified != response else None,
            reason="; ".join(flags) if flags else "Response is safe"
        )
    
    def check_proactive_question(self, question: str) -> SafetyCheck:
        """
        Ki·ªÉm tra c√¢u h·ªèi ch·ªß ƒë·ªông tr∆∞·ªõc khi g·ª≠i.
        
        Args:
            question: C√¢u h·ªèi ƒë·ªãnh g·ª≠i
            
        Returns:
            SafetyCheck
        """
        flags = []
        question_lower = question.lower()
        
        # Check prohibited questions
        for prohibited in self.PROHIBITED_PROACTIVE_QUESTIONS:
            if prohibited in question_lower:
                flags.append(f"Prohibited question topic: {prohibited}")
        
        # Check for too personal questions
        personal_patterns = [
            r"b·∫°n c√≥ b·ªã .+ kh√¥ng\?",
            r"b·∫°n ƒë√£ t·ª´ng .+ ch∆∞a\?",
            r"gia ƒë√¨nh b·∫°n c√≥ ai .+\?",
        ]
        
        for pattern in personal_patterns:
            if re.search(pattern, question_lower):
                flags.append("Too personal question")
        
        is_safe = len(flags) == 0
        
        return SafetyCheck(
            is_safe=is_safe,
            risk_level=RiskLevel.MEDIUM if flags else RiskLevel.NONE,
            flags=flags,
            requires_action=not is_safe,
            action_type="block" if flags else None,
            modified_content=None,
            reason="; ".join(flags) if flags else "Question is appropriate"
        )
    
    def add_appropriate_disclaimer(
        self,
        response: str,
        context_type: str = "general"
    ) -> str:
        """
        Th√™m disclaimer ph√π h·ª£p v√†o response.
        
        Args:
            response: Ph·∫£n h·ªìi g·ªëc
            context_type: Lo·∫°i ng·ªØ c·∫£nh (general, medical, diagnosis)
            
        Returns:
            Ph·∫£n h·ªìi v·ªõi disclaimer
        """
        if context_type == "medical":
            disclaimer = self.BOUNDARY_REMINDER
        else:
            disclaimer = self.SOFT_DISCLAIMER
        
        # Don't add if already present
        if disclaimer in response or "L∆∞u √Ω:" in response:
            return response
        
        return response + "\n" + disclaimer
    
    def get_crisis_response(self, intensity: str = "high") -> str:
        """
        L·∫•y crisis response ph√π h·ª£p.
        
        Args:
            intensity: M·ª©c ƒë·ªô (medium, high, critical)
            
        Returns:
            Crisis response text
        """
        return self.CRISIS_RESPONSE
    
    def sanitize_response(self, response: str) -> str:
        """
        L√†m s·∫°ch response, lo·∫°i b·ªè n·ªôi dung kh√¥ng ph√π h·ª£p.
        
        Args:
            response: Ph·∫£n h·ªìi g·ªëc
            
        Returns:
            Ph·∫£n h·ªìi ƒë√£ l√†m s·∫°ch
        """
        sanitized = response
        
        # Remove any diagnostic language
        replacements = [
            (r"B·∫°n b·ªã (b·ªánh |r·ªëi lo·∫°n )", "C√≥ th·ªÉ b·∫°n ƒëang tr·∫£i qua "),
            (r"B·∫°n m·∫Øc ", "B·∫°n c√≥ th·ªÉ ƒëang g·∫∑p "),
            (r"Ch·∫©n ƒëo√°n c·ªßa b·∫°n l√†", "M·ªôt s·ªë d·∫•u hi·ªáu b·∫°n m√¥ t·∫£ c√≥ th·ªÉ li√™n quan ƒë·∫øn"),
        ]
        
        for pattern, replacement in replacements:
            sanitized = re.sub(pattern, replacement, sanitized, flags=re.IGNORECASE)
        
        return sanitized
    
    async def review_with_llm(self, response: str, user_message: str) -> SafetyCheck:
        """
        Review n·ªôi dung s·ª≠ d·ª•ng LLM (cho tr∆∞·ªùng h·ª£p ph·ª©c t·∫°p).
        
        Args:
            response: Ph·∫£n h·ªìi c·∫ßn review
            user_message: Tin nh·∫Øn ng∆∞·ªùi d√πng
            
        Returns:
            SafetyCheck
        """
        if not self.model:
            return self.check_bot_response(response, user_message)
        
        try:
            review_prompt = f"""ƒê√°nh gi√° ph·∫£n h·ªìi sau ƒë√¢y v·ªÅ m·∫∑t an to√†n v√† ƒë·∫°o ƒë·ª©c.

TIN NH·∫ÆN NG∆Ø·ªúI D√ôNG:
{user_message}

PH·∫¢N H·ªíI C·ª¶A BOT:
{response}

Ki·ªÉm tra:
1. C√≥ ƒë∆∞a ra ch·∫©n ƒëo√°n y t·∫ø kh√¥ng?
2. C√≥ khuy√™n d√πng thu·ªëc kh√¥ng?
3. C√≥ n·ªôi dung c√≥ th·ªÉ g√¢y h·∫°i kh√¥ng?
4. C√≥ ph√π h·ª£p v·ªõi c·∫£m x√∫c ng∆∞·ªùi d√πng kh√¥ng?
5. C√≥ c·∫ßn th√™m disclaimer kh√¥ng?

Tr·∫£ v·ªÅ JSON:
{{
    "is_safe": true/false,
    "concerns": ["concern1", "concern2"],
    "needs_disclaimer": true/false,
    "suggested_modifications": "..." (n·∫øu c·∫ßn)
}}"""
            
            result = self.model.generate_content(review_prompt)
            
            import json
            response_text = result.text.strip()
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0]
            
            data = json.loads(response_text.strip())
            
            concerns = data.get("concerns", [])
            is_safe = data.get("is_safe", True)
            
            modified = response
            if data.get("needs_disclaimer"):
                modified = self.add_appropriate_disclaimer(response)
            if data.get("suggested_modifications"):
                modified = data["suggested_modifications"]
            
            return SafetyCheck(
                is_safe=is_safe,
                risk_level=RiskLevel.LOW if concerns else RiskLevel.NONE,
                flags=concerns,
                requires_action=not is_safe or data.get("needs_disclaimer", False),
                action_type="modify" if modified != response else None,
                modified_content=modified if modified != response else None,
                reason="; ".join(concerns) if concerns else "LLM review passed"
            )
            
        except Exception as e:
            logger.error(f"‚ùå LLM review error: {e}")
            return self.check_bot_response(response, user_message)
    
    def process_response(
        self,
        response: str,
        user_message: str,
        use_llm_review: bool = False
    ) -> Tuple[str, SafetyCheck]:
        """
        X·ª≠ l√Ω ho√†n ch·ªânh m·ªôt response.
        
        Args:
            response: Ph·∫£n h·ªìi g·ªëc
            user_message: Tin nh·∫Øn ng∆∞·ªùi d√πng
            use_llm_review: S·ª≠ d·ª•ng LLM review kh√¥ng
            
        Returns:
            (processed_response, safety_check)
        """
        # Step 1: Basic check
        check = self.check_bot_response(response, user_message)
        
        # Step 2: Sanitize
        sanitized = self.sanitize_response(response)
        
        # Step 3: Apply modifications if needed
        final_response = sanitized
        if check.modified_content:
            final_response = check.modified_content
        
        # Step 4: Final check
        final_check = self.check_bot_response(final_response, user_message)
        
        return final_response, final_check


# Emergency resources by region
EMERGENCY_RESOURCES = {
    "vietnam": {
        "hotline": "1800 599 920",
        "mental_health": "1900 0027",
        "emergency": "115",
        "description": "ƒê∆∞·ªùng d√¢y h·ªó tr·ª£ t√¢m l√Ω mi·ªÖn ph√≠ 24/7"
    },
    "international": {
        "hotline": "International Association for Suicide Prevention",
        "website": "https://www.iasp.info/resources/Crisis_Centres/",
        "description": "Find local crisis centers"
    }
}
